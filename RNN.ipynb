{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLz0Oh2kmp7oDvE5px1Lwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satheesh3180/load_breast_cancer-/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpnXwrtx2bmn",
        "outputId": "78ed575f-3d01-4a9f-a35e-e6a9242adaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "ðŸ§©Standard Neural Network (No memory between words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.5929 - loss: 0.6525 - val_accuracy: 0.8332 - val_loss: 0.3709\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "Review: The movie was fantastic | Predicted: Negative ðŸ˜ž\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Review: The movie was not fantastic | Predicted: Negative ðŸ˜ž\n",
            "ðŸ‘‰ This model fails to understand 'not fantastic' because it treats words separately (no sequence memory).\n",
            "\n",
            "ðŸ§©SimpleRNN remembers previous words (sequential flow)\n",
            "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 100ms/step - accuracy: 0.5328 - loss: 0.6895 - val_accuracy: 0.6336 - val_loss: 0.6422\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\n",
            "Review: The movie is not good | Predicted: Negative ðŸ˜ž\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Review: The movie was fantastic,good movie | Predicted: Negative ðŸ˜ž\n",
            "ðŸ‘‰ RNN slightly understands order, but struggles with long sentences (memory fades).\n",
            "\n",
            "ðŸ§©Vanishing Gradient (Deep SimpleRNN)\n",
            "\u001b[1m44/44\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 459ms/step - accuracy: 0.5006 - loss: 0.7002 - val_accuracy: 0.5532 - val_loss: 0.6862\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
            "Review: The movie started great but ended boring | Predicted: Negative ðŸ˜ž\n",
            "ðŸ‘‰ Deep RNN loses gradient â†’ struggles to capture early info (vanishing gradient).\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Recurrent Neural Networks (RNNs) & LSTMs\n",
        "\n",
        "# 1.Why Standard Neural Networks Fail on Sequences\n",
        "# 2.Architecture and Flow\n",
        "# 3.Training RNNs and Vanishing Gradient Problem\n",
        "# ======================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, Flatten, SimpleRNN, LSTM, GRU, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import numpy as np, random\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Data Loading and Preprocessing\n",
        "# ---------------------------\n",
        "num_words = 10000\n",
        "maxlen = 200\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "\n",
        "def encode(text):\n",
        "    return sequence.pad_sequences([[word_index.get(w, 2) for w in text.lower().split()]], maxlen=maxlen)\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Session 1ï¸âƒ£ - Why Standard Neural Networks Fail on Sequences\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nðŸ§©Standard Neural Network (No memory between words)\")\n",
        "model_dense = Sequential([\n",
        "    Embedding(num_words, 32, input_length=maxlen),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_dense.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_dense.fit(X_train, y_train, epochs=1, batch_size=256, validation_split=0.1, verbose=1)\n",
        "\n",
        "samples = [\"The movie was fantastic\", \"The movie was not fantastic\"]\n",
        "for text in samples:\n",
        "    pred = model_dense.predict(encode(text))[0][0]\n",
        "    print(f\"Review: {text} | Predicted: {'Positive ðŸ˜€' if pred>0.5 else 'Negative ðŸ˜ž'}\")\n",
        "\n",
        "print(\"ðŸ‘‰ This model fails to understand 'not fantastic' because it treats words separately (no sequence memory).\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Session 2ï¸âƒ£ - How RNNs Work: Architecture and Flow\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nðŸ§©SimpleRNN remembers previous words (sequential flow)\")\n",
        "model_rnn = Sequential([\n",
        "    Embedding(num_words, 32, input_length=maxlen),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_rnn.fit(X_train, y_train, epochs=1, batch_size=256, validation_split=0.1, verbose=1)\n",
        "\n",
        "samples = [\"The movie is not good\", \"The movie was fantastic,good movie\"]\n",
        "for text in samples:\n",
        "    pred = model_rnn.predict(encode(text))[0][0]\n",
        "    print(f\"Review: {text} | Predicted: {'Positive ðŸ˜€' if pred>0.5 else 'Negative ðŸ˜ž'}\")\n",
        "\n",
        "print(\"ðŸ‘‰ RNN slightly understands order, but struggles with long sentences (memory fades).\")\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Session 3ï¸âƒ£ - Training RNNs and Vanishing Gradient Problem\n",
        "# --------------------------------------------------------\n",
        "print(\"\\nðŸ§©Vanishing Gradient (Deep SimpleRNN)\")\n",
        "model_vanish = Sequential([\n",
        "    Embedding(num_words, 32, input_length=maxlen),\n",
        "    SimpleRNN(64, return_sequences=True),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_vanish.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_vanish.fit(X_train, y_train, epochs=1, batch_size=512, validation_split=0.1, verbose=1)\n",
        "\n",
        "text = \"The movie started great but ended boring\"\n",
        "pred = model_vanish.predict(encode(text))[0][0]\n",
        "print(f\"Review: {text} | Predicted: {'Positive ðŸ˜€' if pred>0.5 else 'Negative ðŸ˜ž'}\")\n",
        "print(\"ðŸ‘‰ Deep RNN loses gradient â†’ struggles to capture early info (vanishing gradient).\")"
      ]
    }
  ]
}