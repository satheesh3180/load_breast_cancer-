{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFYu3CNGcnURPwtxRaswHJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satheesh3180/load_breast_cancer-/blob/main/PROJECT_FINAL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "def generate_multivariate_series(n_steps=1500):\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    trend = 0.001 * t\n",
        "    seasonality = np.sin(2 * np.pi * t / 24)\n",
        "    temperature = np.sin(2 * np.pi * t / 365)\n",
        "    volatility = np.random.normal(0, 0.2, n_steps).cumsum()\n",
        "    industrial = 0.5 * seasonality + 0.3 * volatility\n",
        "\n",
        "    target = (\n",
        "        trend + seasonality + temperature +\n",
        "        industrial + np.random.normal(0, 0.1, n_steps)\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"target\": target,\n",
        "        \"seasonality\": seasonality,\n",
        "        \"temperature\": temperature,\n",
        "        \"industrial\": industrial,\n",
        "        \"volatility\": volatility\n",
        "    })\n",
        "\n",
        "data = generate_multivariate_series()\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwVBTESoKBN4",
        "outputId": "efe7bbf4-bfb1-40df-adf0-0bda469c86e6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1500, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 3. Sequence Creation\n",
        "\n",
        "def create_sequences(df, seq_len=48):\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - seq_len):\n",
        "        X.append(df.iloc[i:i+seq_len].values)\n",
        "        y.append(df.iloc[i+seq_len][\"target\"])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(data)\n",
        "\n",
        "X, y = create_sequences(pd.DataFrame(scaled, columns=data.columns))\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "d7cWDKmWMdh-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 4. Transformer with Attention (FROM SCRATCH)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim=64, heads=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, attn_weights = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        return x, attn_weights"
      ],
      "metadata": {
        "id": "gVTdqNiCMmkP"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 5. Forecasting Model\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(n_features, 64)\n",
        "        self.transformer = TransformerBlock()\n",
        "        self.fc = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(1, 0, 2)   # (seq, batch, features)\n",
        "        x, attn = self.transformer(x)\n",
        "        out = self.fc(x[-1])\n",
        "        return out.squeeze(), attn"
      ],
      "metadata": {
        "id": "pLtzB3vIMtYC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Encoderâ€“Decoder Transformer\n",
        "\n",
        "#Padding & Masking\n",
        "\n",
        "#Hyperparameter Optimization**\n",
        "\n",
        "#1. Padding + Masking utilities\n",
        "def create_padding_mask(seq):\n",
        "    # seq shape: (batch, seq_len, features)\n",
        "    # padding assumed as zeros\n",
        "    return (seq.abs().sum(dim=-1) == 0)  # (batch, seq_len)\n",
        "\n",
        "#Custom Transformer Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_out, attn_weights = self.attn(\n",
        "            x, x, x, key_padding_mask=mask\n",
        "        )\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        return x, attn_weights\n",
        "\n",
        "#Custom Transformer Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, heads, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, enc_out, mask):\n",
        "        out, _ = self.attn(\n",
        "            x, enc_out, enc_out, key_padding_mask=mask\n",
        "        )\n",
        "        return self.norm(out)\n",
        "\n",
        "#Full Encoderâ€“Decoder Model\n",
        "\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, n_features, embed_dim, heads):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(n_features, embed_dim)\n",
        "        self.encoder = EncoderBlock(embed_dim, heads)\n",
        "        self.decoder = DecoderBlock(embed_dim, heads)\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, src):\n",
        "        mask = create_padding_mask(src)\n",
        "        x = self.embedding(src)\n",
        "\n",
        "        enc_out, attn_weights = self.encoder(x, mask)\n",
        "        dec_out = self.decoder(enc_out[:, -1:].clone(), enc_out, mask)\n",
        "\n",
        "        out = self.fc(dec_out.squeeze(1))\n",
        "        return out.squeeze(), attn_weights"
      ],
      "metadata": {
        "id": "wkDRXRuJPWGm"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#. Hyperparameter Optimization (Grid Search)\n",
        "#Parameters Tuned (as per question)\n",
        "\n",
        "#Learning rate\n",
        "\n",
        "#Sequence length\n",
        "\n",
        "#Number of attention heads\n",
        "\n",
        "param_grid = {\n",
        "    \"lr\": [0.001, 0.0005],\n",
        "    \"seq_len\": [24, 48],\n",
        "    \"heads\": [2, 4]\n",
        "}\n"
      ],
      "metadata": {
        "id": "NgreTUWCPVeW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training + Grid Search Loop\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "best_params = None\n",
        "\n",
        "for lr in param_grid[\"lr\"]:\n",
        "    for seq_len in param_grid[\"seq_len\"]:\n",
        "        for heads in param_grid[\"heads\"]:\n",
        "\n",
        "            X, y = create_sequences(\n",
        "                pd.DataFrame(scaled, columns=data.columns),\n",
        "                seq_len\n",
        "            )\n",
        "\n",
        "            X = torch.tensor(X, dtype=torch.float32)\n",
        "            y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "            loader = DataLoader(\n",
        "                TensorDataset(X, y), batch_size=32, shuffle=True\n",
        "            )\n",
        "\n",
        "            model = TimeSeriesTransformer(\n",
        "                n_features=X.shape[2],\n",
        "                embed_dim=64,\n",
        "                heads=heads\n",
        "            )\n",
        "\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "            loss_fn = nn.MSELoss()\n",
        "\n",
        "            total_loss = 0\n",
        "            for xb, yb in loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds, _ = model(xb)\n",
        "                loss = loss_fn(preds, yb)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(loader)\n",
        "\n",
        "            print(\n",
        "                f\"LR={lr}, Seq={seq_len}, Heads={heads} â†’ Loss={avg_loss:.4f}\"\n",
        "            )\n",
        "\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                best_params = (lr, seq_len, heads)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG5Grxb6QMX5",
        "outputId": "3fce2bd2-ef67-41ed-d025-78218700f250"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR=0.001, Seq=24, Heads=2 â†’ Loss=0.2317\n",
            "LR=0.001, Seq=24, Heads=4 â†’ Loss=0.2285\n",
            "LR=0.001, Seq=48, Heads=2 â†’ Loss=0.2195\n",
            "LR=0.001, Seq=48, Heads=4 â†’ Loss=0.2564\n",
            "LR=0.0005, Seq=24, Heads=2 â†’ Loss=0.2456\n",
            "LR=0.0005, Seq=24, Heads=4 â†’ Loss=0.3691\n",
            "LR=0.0005, Seq=48, Heads=2 â†’ Loss=0.3460\n",
            "LR=0.0005, Seq=48, Heads=4 â†’ Loss=0.2164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Best Hyperparameters (Required Deliverable)\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found:\")\n",
        "print(\"Learning Rate:\", best_params[0])\n",
        "print(\"Sequence Length:\", best_params[1])\n",
        "print(\"Attention Heads:\", best_params[2])\n",
        "print(\"Best Loss:\", best_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBMiFDWzQTYp",
        "outputId": "c2928c19-c71e-49fa-c07c-38c5947ef3c0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters Found:\n",
            "Learning Rate: 0.0005\n",
            "Sequence Length: 48\n",
            "Attention Heads: 4\n",
            "Best Loss: 0.21640216769731563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 6. Training Loop\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Use embed_dim=64 as it was used in the hyperparameter search, and best_params[2] for heads\n",
        "model = TimeSeriesTransformer(n_features=X.shape[2], embed_dim=64, heads=best_params[2]).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) # Keeping the original learning rate from this cell\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds, attn = model(xb)\n",
        "        loss = loss_fn(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpBONuByMznR",
        "outputId": "819b60b2-265b-44d6-92a5-4b40a7b70e54"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.2125\n",
            "Epoch 2 | Loss: 0.0552\n",
            "Epoch 3 | Loss: 0.0422\n",
            "Epoch 4 | Loss: 0.0407\n",
            "Epoch 5 | Loss: 0.0362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 7. Evaluation Metrics\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds, attn = model(X.to(device))\n",
        "\n",
        "preds = preds.cpu().numpy()\n",
        "actual = y.numpy()\n",
        "\n",
        "mae = mean_absolute_error(actual, preds)\n",
        "rmse = np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"Attention weights shape:\", attn.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKwpqBMTNKEX",
        "outputId": "8cff8591-b080-4c68-eca1-ff42f3189caa"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.15098091959953308\n",
            "RMSE: 0.17781986025240762\n",
            "Attention weights shape: torch.Size([1452, 48, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ”¹ 8. SARIMAX Baseline (Comparison)\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "sarimax = SARIMAX(data[\"target\"], order=(2,1,2))\n",
        "sarimax_fit = sarimax.fit(disp=False)\n",
        "\n",
        "print(\"SARIMAX AIC:\", sarimax_fit.aic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwuVDPjrNTk3",
        "outputId": "5e97d0f8-ff57-4417-dfc7-420fe78b4bbf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARIMAX AIC: -1280.9912060148326\n"
          ]
        }
      ]
    }
  ]
}